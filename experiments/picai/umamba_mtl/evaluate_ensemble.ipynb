{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<frozen importlib._bootstrap_external>:1328: FutureWarning: The cuda.cudart module is deprecated and will be removed in a future release, please switch to use the cuda.bindings.runtime module instead.\n",
      "`torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "`torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "If you have questions or suggestions, feel free to open an issue at https://github.com/DIAGNijmegen/picai_eval\n",
      "\n",
      "\n",
      "\n",
      "Please cite the following paper when using Report Guided Annotations:\n",
      "\n",
      "Bosma, J.S., et al. \"Semi-supervised learning with report-guided lesion annotation for deep learning-based prostate cancer detection in bpMRI\" to be submitted\n",
      "\n",
      "\n",
      "If you have questions or suggestions, feel free to open an issue at https://github.com/DIAGNijmegen/Report-Guided-Annotation\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from trainer import LitModel\n",
    "import torch \n",
    "from shared_modules.data_module_all import DataModule\n",
    "from shared_modules.utils import load_config\n",
    "from shared_modules.plotting import plot_metrics, plot_confusion, plot_difference\n",
    "from shared_modules.torch_metrics import PicaiMetric\n",
    "from shared_modules.post_transforms import get_post_transforms\n",
    "from monai.transforms import AsDiscrete\n",
    "from tqdm import tqdm\n",
    "from monai.metrics import DiceMetric\n",
    "from monai.data import decollate_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVE_PROB_MAPS = False\n",
    "SAVE_PREDS = False\n",
    "\n",
    "config = load_config(\"config.yaml\")\n",
    "config.data.data_dir = \"../../../data/\"\n",
    "config.data.json_list = \"../../../json_datalists/picai/all_samples.json\"\n",
    "gpu = 0\n",
    "config.gpus = [gpu]\n",
    "config.cache_rate = 1.0\n",
    "config.transforms.label_keys=[\"pca\", \"prostate\"]\n",
    "config.transforms.crop_key = \"prostate\"\n",
    "# config.transforms.image_keys = [\"t2w\", \"adc\", \"hbv\"]\n",
    "config.transforms.image_keys = [\"image\"]\n",
    "label_key = config.transforms.label_keys[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_folder = \"../../../gc_algorithms/base_container/models/umamba_mtl/weights/\"\n",
    "models = []\n",
    "\n",
    "for i in range(5):\n",
    "    models.append(LitModel.load_from_checkpoint(f\"{weights_folder}f{i}.ckpt\", config=config, map_location=f\"cuda:{gpu}\"))\n",
    "    # disable randomness, dropout, etc...\n",
    "    models[-1].eval()\n",
    "    models[-1].to(gpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "monai.transforms.spatial.dictionary Orientationd.__init__:labels: Current default value of argument `labels=(('L', 'R'), ('P', 'A'), ('I', 'S'))` was changed in version None from `labels=(('L', 'R'), ('P', 'A'), ('I', 'S'))` to `labels=None`. Default value changed to None meaning that the transform now uses the 'space' of a meta-tensor, if applicable, to determine appropriate axis labels.\n",
      "Loading dataset: 100%|██████████| 1499/1499 [03:01<00:00,  8.27it/s]\n"
     ]
    }
   ],
   "source": [
    "dm = DataModule(\n",
    "    config=config,\n",
    ")\n",
    "\n",
    "dm.setup(\"test\")\n",
    "dl = [next(iter(dm.test_dataloader()))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_map_post_transforms = get_post_transforms(key=\"prob\", \n",
    "                    orig_key=label_key,\n",
    "                    orig_transforms=dm.transforms[\"test\"],\n",
    "                    out_dir=f\"output/prob/\",\n",
    "                    keep_n_largest_components=0,\n",
    "                    output_postfix=\"\",\n",
    "                    output_dtype=\"float32\",\n",
    "                    save_mask=SAVE_PROB_MAPS) # True\n",
    "\n",
    "pca_post_transforms = get_post_transforms(key=\"pca\", \n",
    "                    orig_key=label_key,\n",
    "                    orig_transforms=dm.transforms[\"test\"],\n",
    "                    out_dir=f\"output/pred/\",\n",
    "                    keep_n_largest_components=0,\n",
    "                    output_postfix=\"\",\n",
    "                    output_dtype=\"float32\",\n",
    "                    save_mask=SAVE_PREDS) # True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:10<00:00, 10.14s/it]\n"
     ]
    }
   ],
   "source": [
    "from captum.attr import Occlusion, FeatureAblation, LayerGradCam\n",
    "from captum.attr import visualization as viz\n",
    "\n",
    "model = models[0]\n",
    "\n",
    "for image in tqdm(dl):\n",
    "    x = image[\"image\"].to(0)\n",
    "    logits = model(x)\n",
    "    pred = AsDiscrete(argmax=True)(logits[0])[None].cpu()\n",
    "\n",
    "def agg_segmentation_wrapper(image):\n",
    "    pass\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:09<?, ?it/s]\n"
     ]
    },
    {
     "ename": "AcceleratorError",
     "evalue": "CUDA error: invalid device ordinal\nGPU device may be out of range, do you have enough GPUs?\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAcceleratorError\u001b[39m                          Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 36\u001b[39m\n\u001b[32m     31\u001b[39m futures = [\n\u001b[32m     32\u001b[39m     executor.submit(run_model, fold, model, gpus[fold], x) \n\u001b[32m     33\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m fold, model \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(models)\n\u001b[32m     34\u001b[39m ]\n\u001b[32m     35\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m future \u001b[38;5;129;01min\u001b[39;00m futures:\n\u001b[32m---> \u001b[39m\u001b[32m36\u001b[39m     pred, prob = \u001b[43mfuture\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     37\u001b[39m     preds.append(pred)\n\u001b[32m     38\u001b[39m     probs.append(prob)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/uv/python/cpython-3.13.7-linux-x86_64-gnu/lib/python3.13/concurrent/futures/_base.py:449\u001b[39m, in \u001b[36mFuture.result\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    447\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[32m    448\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state == FINISHED:\n\u001b[32m--> \u001b[39m\u001b[32m449\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    451\u001b[39m \u001b[38;5;28mself\u001b[39m._condition.wait(timeout)\n\u001b[32m    453\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/uv/python/cpython-3.13.7-linux-x86_64-gnu/lib/python3.13/concurrent/futures/_base.py:401\u001b[39m, in \u001b[36mFuture.__get_result\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    399\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    400\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m401\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception\n\u001b[32m    402\u001b[39m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    403\u001b[39m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[32m    404\u001b[39m         \u001b[38;5;28mself\u001b[39m = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/uv/python/cpython-3.13.7-linux-x86_64-gnu/lib/python3.13/concurrent/futures/thread.py:59\u001b[39m, in \u001b[36m_WorkItem.run\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     56\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m     58\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m59\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     60\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m     61\u001b[39m     \u001b[38;5;28mself\u001b[39m.future.set_exception(exc)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 24\u001b[39m, in \u001b[36mrun_model\u001b[39m\u001b[34m(fold, model, gpu, x)\u001b[39m\n\u001b[32m     23\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mrun_model\u001b[39m(fold, model, gpu, x):\n\u001b[32m---> \u001b[39m\u001b[32m24\u001b[39m     x_gpu = \u001b[43mx\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgpu\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     25\u001b[39m     logits = model.inferer(x_gpu)\n\u001b[32m     26\u001b[39m     pred = AsDiscrete(argmax=\u001b[38;5;28;01mTrue\u001b[39;00m)(logits[\u001b[32m0\u001b[39m])[\u001b[38;5;28;01mNone\u001b[39;00m].cpu()  \u001b[38;5;66;03m# Move back to CPU\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/U-MambaMTL-XAI/.venv/lib/python3.13/site-packages/monai/data/meta_tensor.py:283\u001b[39m, in \u001b[36mMetaTensor.__torch_function__\u001b[39m\u001b[34m(cls, func, types, args, kwargs)\u001b[39m\n\u001b[32m    281\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m kwargs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    282\u001b[39m     kwargs = {}\n\u001b[32m--> \u001b[39m\u001b[32m283\u001b[39m ret = \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43m__torch_function__\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtypes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    284\u001b[39m \u001b[38;5;66;03m# if `out` has been used as argument, metadata is not copied, nothing to do.\u001b[39;00m\n\u001b[32m    285\u001b[39m \u001b[38;5;66;03m# if \"out\" in kwargs:\u001b[39;00m\n\u001b[32m    286\u001b[39m \u001b[38;5;66;03m#     return ret\u001b[39;00m\n\u001b[32m    287\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m _not_requiring_metadata(ret):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/U-MambaMTL-XAI/.venv/lib/python3.13/site-packages/torch/_tensor.py:1703\u001b[39m, in \u001b[36mTensor.__torch_function__\u001b[39m\u001b[34m(cls, func, types, args, kwargs)\u001b[39m\n\u001b[32m   1700\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mNotImplemented\u001b[39m\n\u001b[32m   1702\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m _C.DisableTorchFunctionSubclass():\n\u001b[32m-> \u001b[39m\u001b[32m1703\u001b[39m     ret = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1704\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m func \u001b[38;5;129;01min\u001b[39;00m get_default_nowrap_functions():\n\u001b[32m   1705\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m ret\n",
      "\u001b[31mAcceleratorError\u001b[39m: CUDA error: invalid device ordinal\nGPU device may be out of range, do you have enough GPUs?\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "\n",
    "# After loading models, distribute them across GPUs\n",
    "gpus = [0, 1, 2, 3, 4]\n",
    "dsc_fn = DiceMetric(include_background=False, reduction=\"mean\")\n",
    "picai_metric_fn = PicaiMetric()\n",
    "\n",
    "all_probs = []\n",
    "all_gts = []\n",
    "\n",
    "# Use the first GPU for metric computation\n",
    "metric_gpu = gpus[0]\n",
    "\n",
    "for batch in tqdm(dl):\n",
    "    with torch.no_grad():\n",
    "        x = batch[\"image\"]\n",
    "        \n",
    "        preds = []\n",
    "        probs = []\n",
    "        \n",
    "        def run_model(fold, model, gpu, x):\n",
    "            x_gpu = x.to(gpu)\n",
    "            logits = model.inferer(x_gpu)\n",
    "            pred = AsDiscrete(argmax=True)(logits[0])[None].cpu()  # Move back to CPU\n",
    "            prob = torch.sigmoid(logits[0,1])[None][None].cpu()\n",
    "            return pred, prob\n",
    "        \n",
    "        with ThreadPoolExecutor(max_workers=5) as executor:\n",
    "            futures = [\n",
    "                executor.submit(run_model, fold, model, gpus[fold], x) \n",
    "                for fold, model in enumerate(models)\n",
    "            ]\n",
    "            for future in futures:\n",
    "                pred, prob = future.result()\n",
    "                preds.append(pred)\n",
    "                probs.append(prob)\n",
    "        \n",
    "    batch[\"prob\"] = torch.mean(torch.stack(probs), dim=0)\n",
    "    \n",
    "    # Reverts back to original size\n",
    "    batch[\"prob\"] = [prob_map_post_transforms(i)[\"prob\"] for i in decollate_batch(batch)][0][None]\n",
    "    batch[\"pca\"] = [pca_post_transforms(i)[\"pca\"] for i in decollate_batch(batch)][0][None]\n",
    "    batch[\"pred\"] = (batch[\"prob\"] > 0.5).float()\n",
    "     \n",
    "    all_probs.append(batch[\"prob\"][0,0,...].cpu().numpy())\n",
    "    all_gts.append(batch[\"pca\"][0,0,...].cpu().numpy())\n",
    "    \n",
    "    # Keep both on same device for metric computation\n",
    "    dsc_fn(y_pred=batch[\"pred\"].to(metric_gpu), y=batch[\"pca\"].to(metric_gpu))\n",
    "        \n",
    "dsc_metrics = dsc_fn.aggregate(\"none\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:01<00:00,  1.28s/it]\n"
     ]
    }
   ],
   "source": [
    "dsc_fn = DiceMetric(include_background=False, reduction=\"mean\")\n",
    "picai_metric_fn = PicaiMetric()\n",
    "\n",
    "all_probs = []\n",
    "all_gts = []\n",
    "\n",
    "\n",
    "for batch in tqdm(dl):\n",
    "    with torch.no_grad():\n",
    "        x = batch[\"image\"].to(gpu)\n",
    "\n",
    "        preds = []\n",
    "        probs = []\n",
    "        \n",
    "        for fold, model in enumerate(models):\n",
    "            logits = model.inferer(x)\n",
    "            preds.append(AsDiscrete(argmax=True)(logits[0])[None])\n",
    "            probs.append(torch.sigmoid(logits[0,1])[None][None])\n",
    "            \n",
    "        \n",
    "    batch[\"prob\"] = torch.mean(torch.stack(probs), dim=0)\n",
    "    \n",
    "    # Reverts back to original size\n",
    "    batch[\"prob\"] = [prob_map_post_transforms(i)[\"prob\"] for i in decollate_batch(batch)][0][None]\n",
    "    batch[\"pca\"] = [pca_post_transforms(i)[\"pca\"] for i in decollate_batch(batch)][0][None]\n",
    "    batch[\"pred\"] = (batch[\"prob\"] > 0.5).float()\n",
    "     \n",
    "    all_probs.append(batch[\"prob\"][0,0,...].cpu().numpy())\n",
    "    all_gts.append(batch[\"pca\"][0,0,...].cpu().numpy())\n",
    "    \n",
    "    dsc_fn(y_pred=batch[\"pred\"], y=batch[\"pca\"].to(gpu))\n",
    "        \n",
    "dsc_metrics = dsc_fn.aggregate(\"none\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Metrics(auroc=nan%, AP=-0.00%, 1 cases, 0 lesions)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import picai_eval\n",
    "from report_guided_annotation import extract_lesion_candidates\n",
    "\n",
    "metrics = picai_eval.evaluate(\n",
    "            y_det=all_probs,\n",
    "            y_true=all_gts,\n",
    "            y_det_postprocess_func=lambda pred: extract_lesion_candidates(pred, threshold=\"dynamic\")[0],\n",
    "            y_true_postprocess_func=lambda y: y,\n",
    "            num_parallel_calls=10\n",
    "        )\n",
    "\n",
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_metrics(metrics,56)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion(metrics, 56, threshold=0.5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "U-MambaMTL-XAI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
